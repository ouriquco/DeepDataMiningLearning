{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from skimage import io\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextStem(nn.Sequential):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__(\n",
    "            ConvNormAct(\n",
    "                in_features, out_features, kernel_size=7, stride=2\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        stem_features: int,\n",
    "        depths: List[int],\n",
    "        widths: List[int],\n",
    "        drop_p: float = .0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stem = ConvNextStem(in_channels, stem_features)\n",
    "\n",
    "        in_out_widths = list(zip(widths, widths[1:]))\n",
    "        # create drop paths probabilities (one for each stage)\n",
    "        drop_probs = [x.item() for x in torch.linspace(0, drop_p, sum(depths))] \n",
    "        \n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                ConvNexStage(stem_features, widths[0], depths[0], drop_p=drop_probs[0]),\n",
    "                *[\n",
    "                    ConvNexStage(in_features, out_features, depth, drop_p=drop_p)\n",
    "                    for (in_features, out_features), depth, drop_p in zip(\n",
    "                        in_out_widths, depths[1:], drop_probs[1:]\n",
    "                    )\n",
    "                ],\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 7, 7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(1, 3, 224, 224)\n",
    "encoder = ConvNextEncoder(in_channels=3, stem_features=64, depths=[3,4,6,4], widths=[256, 512, 1024, 2048])\n",
    "encoder(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConvNextEncoder(in_channels=3, stem_features=64, depths=[3,3,9,3], widths=[256, 512, 1024, 2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextStem(nn.Sequential):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=4, stride=4),\n",
    "            nn.BatchNorm2d(out_features)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import StochasticDepth\n",
    "\n",
    "class LayerScaler(nn.Module):\n",
    "    def __init__(self, init_value: float, dimensions: int):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_value * torch.ones((dimensions)), \n",
    "                                    requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gamma[None,...,None,None] * x\n",
    "\n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        expansion: int = 4,\n",
    "        drop_p: float = .0,\n",
    "        layer_scaler_init_value: float = 1e-6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # narrow -> wide (with depth-wise and bigger kernel)\n",
    "            nn.Conv2d(\n",
    "                in_features, in_features, kernel_size=7, padding=3, bias=False, groups=in_features\n",
    "            ),\n",
    "            # GroupNorm with num_groups=1 is the same as LayerNorm but works for 2D data\n",
    "            nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "            # wide -> wide \n",
    "            nn.Conv2d(in_features, expanded_features, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            # wide -> narrow\n",
    "            nn.Conv2d(expanded_features, out_features, kernel_size=1),\n",
    "        )\n",
    "        self.layer_scaler = LayerScaler(layer_scaler_init_value, out_features)\n",
    "        self.drop_path = StochasticDepth(drop_p, mode=\"batch\")\n",
    "\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        x = self.layer_scaler(x)\n",
    "        x = self.drop_path(x)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNexStage(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, in_features: int, out_features: int, depth: int, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            # add the downsampler\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=2, stride=2)\n",
    "            ),\n",
    "            *[\n",
    "                BottleNeckBlock(out_features, out_features, **kwargs)\n",
    "                for _ in range(depth)\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 62, 7, 7])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage = ConvNexStage(32, 62, depth=1)\n",
    "stage(torch.randn(1, 32, 14, 14)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, num_channels: int, num_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(1),\n",
    "            nn.LayerNorm(num_channels),\n",
    "            nn.Linear(num_channels, num_classes)\n",
    "        )\n",
    "    \n",
    "    \n",
    "class ConvNextForImageClassification(nn.Sequential):\n",
    "    def __init__(self,  \n",
    "                 in_channels: int,\n",
    "                 stem_features: int,\n",
    "                 depths: List[int],\n",
    "                 widths: List[int],\n",
    "                 drop_p: float = .0,\n",
    "                 num_classes: int = 1000):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvNextEncoder(in_channels, stem_features, depths, widths, drop_p)\n",
    "        self.head = ClassificationHead(widths[-1], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(1, 3, 224, 224)\n",
    "classifier = ConvNextForImageClassification(in_channels=3, stem_features=64, depths=[3,4,6,4], widths=[256, 512, 1024, 2048])\n",
    "classifier(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified ConvNextStem for KITTI images (typically 375x1242)\n",
    "class ConvNextStem(nn.Sequential):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_features)\n",
    "        )\n",
    "\n",
    "# Modified ConvNext for Object Detection\n",
    "class ConvNextForObjectDetection(nn.Module):\n",
    "    def __init__(self,  \n",
    "                 in_channels: int = 3,  # KITTI uses RGB images\n",
    "                 stem_features: int = 96,\n",
    "                 depths: List[int] = [3, 3, 9, 3],\n",
    "                 widths: List[int] = [96, 192, 384, 768],\n",
    "                 drop_p: float = 0.1,\n",
    "                 num_classes: int = 8):  # KITTI has 8 main classes\n",
    "        super().__init__()\n",
    "        self.encoder = ConvNextEncoder(in_channels, stem_features, depths, widths, drop_p)\n",
    "        \n",
    "        # Detection head instead of classification head\n",
    "        self.detection_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(1),\n",
    "            nn.LayerNorm(widths[-1]),\n",
    "            nn.Linear(widths[-1], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, num_classes * 5)  # 5 values per class (class, x, y, w, h)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        detections = self.detection_head(features)\n",
    "        batch_size = detections.shape[0]\n",
    "        return detections.view(batch_size, -1, 5)  # Reshape to (batch_size, num_classes, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for KITTI dataset\n",
    "class KITTIConfig:\n",
    "    NUM_CLASSES = 8  # Main KITTI classes\n",
    "    IMAGE_SIZE = (375, 1242)  # Standard KITTI image size\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 1e-4\n",
    "    EPOCHS = 100\n",
    "    \n",
    "    # Class mapping\n",
    "    CLASS_MAPPING = {\n",
    "        'Car': 0,\n",
    "        'Van': 1,\n",
    "        'Truck': 2,\n",
    "        'Pedestrian': 3,\n",
    "        'Cyclist': 4,\n",
    "        'Person_sitting': 5,\n",
    "        'Tram': 6,\n",
    "        'Misc': 7\n",
    "    }\n",
    "\n",
    "# Loss function for object detection\n",
    "class DetectionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "        self.regression_loss = nn.SmoothL1Loss()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        class_pred = predictions[..., 0]\n",
    "        bbox_pred = predictions[..., 1:]\n",
    "        class_target = targets[..., 0]\n",
    "        bbox_target = targets[..., 1:]\n",
    "        \n",
    "        class_loss = self.classification_loss(class_pred, class_target)\n",
    "        bbox_loss = self.regression_loss(bbox_pred, bbox_target)\n",
    "        \n",
    "        return class_loss + bbox_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Define paths for images and labels\n",
    "        self.image_dir = os.path.join(root_dir, split, 'image_2')\n",
    "        self.label_dir = os.path.join(root_dir, 'label_2')\n",
    "        \n",
    "        # Get all image files\n",
    "        self.images = sorted(os.listdir(self.image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Load image\n",
    "        img_name = os.path.join(self.image_dir, self.images[idx])\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        # Load labels\n",
    "        label_name = os.path.join(self.label_dir, self.images[idx].replace('.png', '.txt'))\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        with open(label_name, 'r') as f:\n",
    "            for line in f:\n",
    "                data = line.strip().split()\n",
    "                category = data[0]\n",
    "                if category not in ['DontCare']:  # Skip DontCare objects\n",
    "                    bbox = [float(x) for x in data[4:8]]  # [x1, y1, x2, y2]\n",
    "                    boxes.append(bbox)\n",
    "                    labels.append(KITTIConfig.CLASS_MAPPING[category])\n",
    "\n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    \n",
    "    # Find the maximum width and height in the batch\n",
    "    max_height = max(image.size(1) for image in images)\n",
    "    max_width = max(image.size(2) for image in images)\n",
    "\n",
    "    padded_images = []\n",
    "    for image in images:\n",
    "        # Create a new tensor filled with zeros (black image) with max dimensions\n",
    "        padded_image = torch.zeros((image.size(0), max_height, max_width))\n",
    "        # Paste the original image into the padded image\n",
    "        padded_image[:, :image.size(1), :image.size(2)] = image\n",
    "        padded_images.append(padded_image)\n",
    "\n",
    "    # Stack padded images into a single tensor\n",
    "    images = torch.stack(padded_images)\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "# Create train loader\n",
    "train_dataset = KITTIDataset(\n",
    "    root_dir='/data/cmpe258-sp24/010892622/data/KITTI',\n",
    "    split='training',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")\n",
    "# Modified DataLoader configuration\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,  # Reduced batch size to avoid memory issues\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),  # Important for object detection\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=2,\n",
    "#     shuffle=True,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=collate_fn,  # Required for handling variable number of objects\n",
    "#     pin_memory=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, targets in train_loader:\n",
    "            images = images.to(device)  # Move the entire batch to the device\n",
    "            targets = [{k: v.to(device) for k, v in target.items()} for target in targets]  # Move each target to device\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)  # This should now work correctly\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, targets in train_loader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Convert targets to device\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "        self.regression_loss = nn.SmoothL1Loss(reduction='mean')\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Convert list of dictionaries to appropriate tensor format\n",
    "        batch_labels = []\n",
    "        batch_boxes = []\n",
    "        \n",
    "        for target in targets:\n",
    "            batch_labels.append(target['labels'])\n",
    "            batch_boxes.append(target['boxes'])\n",
    "        \n",
    "        # Stack tensors if possible, otherwise handle them individually\n",
    "        try:\n",
    "            target_labels = torch.cat(batch_labels)\n",
    "            target_boxes = torch.cat(batch_boxes)\n",
    "        except:\n",
    "            target_labels = torch.stack([t['labels'][0] for t in targets])\n",
    "            target_boxes = torch.stack([t['boxes'][0] for t in targets])\n",
    "\n",
    "        # Reshape predictions to match target format\n",
    "        pred_class = predictions[..., 0]  # Classification predictions\n",
    "        pred_boxes = predictions[..., 1:]  # Bounding box predictions\n",
    "        \n",
    "        # Calculate losses\n",
    "        cls_loss = self.classification_loss(pred_class, target_labels)\n",
    "        box_loss = self.regression_loss(pred_boxes, target_boxes)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = cls_loss + box_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (2) to match target batch_size (12).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     14\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m, in \u001b[0;36mDetectionLoss.forward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     26\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m predictions[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Bounding box predictions\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate losses\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m cls_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m box_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregression_loss(pred_boxes, target_boxes)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Combine losses\u001b[39;00m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (2) to match target batch_size (12)."
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "model = ConvNextForObjectDetection(\n",
    "    in_channels=3,\n",
    "    stem_features=96,\n",
    "    depths=[3, 3, 9, 3],\n",
    "    widths=[96, 192, 384, 768],\n",
    "    drop_p=0.1,\n",
    "    num_classes=KITTIConfig.NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Initialize the loss and optimizer\n",
    "criterion = DetectionLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "num_epochs = 3\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
