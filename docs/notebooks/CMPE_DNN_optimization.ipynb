{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: timm in /home/010892622/.local/lib/python3.11/site-packages (1.0.11)\n",
      "Requirement already satisfied: torch in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from timm) (2.2.0)\n",
      "Requirement already satisfied: torchvision in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from timm) (0.17.0)\n",
      "Requirement already satisfied: pyyaml in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/010892622/.local/lib/python3.11/site-packages (from timm) (0.26.2)\n",
      "Requirement already satisfied: safetensors in /home/010892622/.local/lib/python3.11/site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from huggingface_hub->timm) (3.12.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from huggingface_hub->timm) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from huggingface_hub->timm) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from huggingface_hub->timm) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torch->timm) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.3.101)\n",
      "Requirement already satisfied: numpy in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from torchvision->timm) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.2.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision # has datasets\n",
    "from timm import create_model\n",
    "from timm.data import resolve_model_data_config\n",
    "from timm.data import create_transform\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')  # CUDA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps') #Apple GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device Name: {device}\")\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(\"Device name: \", torch.cuda.get_device_name(0))\n",
    "    print(\"Device properties:\", torch.cuda.get_device_properties(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'AlexNet_Weights',\n",
       " 'ConvNeXt',\n",
       " 'ConvNeXt_Base_Weights',\n",
       " 'ConvNeXt_Large_Weights',\n",
       " 'ConvNeXt_Small_Weights',\n",
       " 'ConvNeXt_Tiny_Weights',\n",
       " 'DenseNet',\n",
       " 'DenseNet121_Weights',\n",
       " 'DenseNet161_Weights',\n",
       " 'DenseNet169_Weights',\n",
       " 'DenseNet201_Weights',\n",
       " 'EfficientNet',\n",
       " 'EfficientNet_B0_Weights',\n",
       " 'EfficientNet_B1_Weights',\n",
       " 'EfficientNet_B2_Weights',\n",
       " 'EfficientNet_B3_Weights',\n",
       " 'EfficientNet_B4_Weights',\n",
       " 'EfficientNet_B5_Weights',\n",
       " 'EfficientNet_B6_Weights',\n",
       " 'EfficientNet_B7_Weights',\n",
       " 'EfficientNet_V2_L_Weights',\n",
       " 'EfficientNet_V2_M_Weights',\n",
       " 'EfficientNet_V2_S_Weights',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'GoogLeNet_Weights',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'Inception_V3_Weights',\n",
       " 'MNASNet',\n",
       " 'MNASNet0_5_Weights',\n",
       " 'MNASNet0_75_Weights',\n",
       " 'MNASNet1_0_Weights',\n",
       " 'MNASNet1_3_Weights',\n",
       " 'MaxVit',\n",
       " 'MaxVit_T_Weights',\n",
       " 'MobileNetV2',\n",
       " 'MobileNetV3',\n",
       " 'MobileNet_V2_Weights',\n",
       " 'MobileNet_V3_Large_Weights',\n",
       " 'MobileNet_V3_Small_Weights',\n",
       " 'RegNet',\n",
       " 'RegNet_X_16GF_Weights',\n",
       " 'RegNet_X_1_6GF_Weights',\n",
       " 'RegNet_X_32GF_Weights',\n",
       " 'RegNet_X_3_2GF_Weights',\n",
       " 'RegNet_X_400MF_Weights',\n",
       " 'RegNet_X_800MF_Weights',\n",
       " 'RegNet_X_8GF_Weights',\n",
       " 'RegNet_Y_128GF_Weights',\n",
       " 'RegNet_Y_16GF_Weights',\n",
       " 'RegNet_Y_1_6GF_Weights',\n",
       " 'RegNet_Y_32GF_Weights',\n",
       " 'RegNet_Y_3_2GF_Weights',\n",
       " 'RegNet_Y_400MF_Weights',\n",
       " 'RegNet_Y_800MF_Weights',\n",
       " 'RegNet_Y_8GF_Weights',\n",
       " 'ResNeXt101_32X8D_Weights',\n",
       " 'ResNeXt101_64X4D_Weights',\n",
       " 'ResNeXt50_32X4D_Weights',\n",
       " 'ResNet',\n",
       " 'ResNet101_Weights',\n",
       " 'ResNet152_Weights',\n",
       " 'ResNet18_Weights',\n",
       " 'ResNet34_Weights',\n",
       " 'ResNet50_Weights',\n",
       " 'ShuffleNetV2',\n",
       " 'ShuffleNet_V2_X0_5_Weights',\n",
       " 'ShuffleNet_V2_X1_0_Weights',\n",
       " 'ShuffleNet_V2_X1_5_Weights',\n",
       " 'ShuffleNet_V2_X2_0_Weights',\n",
       " 'SqueezeNet',\n",
       " 'SqueezeNet1_0_Weights',\n",
       " 'SqueezeNet1_1_Weights',\n",
       " 'SwinTransformer',\n",
       " 'Swin_B_Weights',\n",
       " 'Swin_S_Weights',\n",
       " 'Swin_T_Weights',\n",
       " 'Swin_V2_B_Weights',\n",
       " 'Swin_V2_S_Weights',\n",
       " 'Swin_V2_T_Weights',\n",
       " 'VGG',\n",
       " 'VGG11_BN_Weights',\n",
       " 'VGG11_Weights',\n",
       " 'VGG13_BN_Weights',\n",
       " 'VGG13_Weights',\n",
       " 'VGG16_BN_Weights',\n",
       " 'VGG16_Weights',\n",
       " 'VGG19_BN_Weights',\n",
       " 'VGG19_Weights',\n",
       " 'ViT_B_16_Weights',\n",
       " 'ViT_B_32_Weights',\n",
       " 'ViT_H_14_Weights',\n",
       " 'ViT_L_16_Weights',\n",
       " 'ViT_L_32_Weights',\n",
       " 'VisionTransformer',\n",
       " 'Weights',\n",
       " 'WeightsEnum',\n",
       " 'Wide_ResNet101_2_Weights',\n",
       " 'Wide_ResNet50_2_Weights',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_api',\n",
       " '_meta',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'convnext',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'efficientnet',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'feature_extraction',\n",
       " 'get_model',\n",
       " 'get_model_builder',\n",
       " 'get_model_weights',\n",
       " 'get_weight',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'list_models',\n",
       " 'maxvit',\n",
       " 'maxvit_t',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenetv2',\n",
       " 'mobilenetv3',\n",
       " 'optical_flow',\n",
       " 'quantization',\n",
       " 'regnet',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_transformer',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'vision_transformer',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at available torchvision models\n",
    "# models with suffix Weights have pretrained version\n",
    "# note we cannot use timm models on HPC because there is an issue with download\n",
    "available_models = dir(torchvision.models)\n",
    "available_models\n",
    "\n",
    "## Available models in timm library\n",
    "# avail_pretrained_models = timm.list_models(pretrained=True)\n",
    "# len(avail_pretrained_models), avail_pretrained_models[:125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transform configuation\n",
    "# Load pretrained model using the latest available weights\n",
    "# Weights have to be manually saved and moved to HPC becasue of network issue\n",
    "model = torchvision.models.convnext_base()\n",
    "model.load_state_dict(torch.load('../../../convnext_base_weights.pth'))\n",
    "model\n",
    "\n",
    "# config parameters for transformation\n",
    "data_config = resolve_model_data_config(model)\n",
    "transforms = create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: /data/cmpe258-sp24/010892622/data/CIFAR10/train\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
       "                CenterCrop(size=(224, 224))\n",
       "                MaybeToTensor()\n",
       "                Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       "            ),\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: /data/cmpe258-sp24/010892622/data/CIFAR10/test\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
       "                CenterCrop(size=(224, 224))\n",
       "                MaybeToTensor()\n",
       "                Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       "            ))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "# '/data/cmpe258-sp24/010892622/data/CIFAR10'\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='/data/cmpe258-sp24/010892622/data/CIFAR10/train', train=True, transform=transforms,download=True)\n",
    "test_set = torchvision.datasets.CIFAR10(root='/data/cmpe258-sp24/010892622/data/CIFAR10/test', train=False, transform=transforms,download=True)\n",
    "train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# Set GPU for model\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and the optimization algorithm\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.classifier[2] = torch.nn.Linear(model.classifier[2].in_features, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Initiate gradient descent\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torch/optim/adam.py:113\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    111\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Begin training loop\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device) # Move features and targets to GPU\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_pred, y)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Reset gradient \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initiate backward propogation\n",
    "        loss.backward()\n",
    "\n",
    "        # Initiate gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('head',\n",
       " NormMlpClassifierHead(\n",
       "   (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "   (norm): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (pre_logits): Identity()\n",
       "   (drop): Dropout(p=0.0, inplace=False)\n",
       "   (fc): Linear(in_features=1024, out_features=21841, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastmodule=list(model.named_children())[-1] #get the last Sequential layer\n",
    "lastmodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select loss function and optimization algorithm \n",
    "from torch import nn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adjust learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(module: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, device: torch.device) -> tuple[float,float]:\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for\n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "              train_acc: [...],\n",
    "              test_loss: [...],\n",
    "              test_acc: [...]}\n",
    "    For example if training for epochs=2:\n",
    "             {train_loss: [2.0616, 1.0537],\n",
    "              train_acc: [0.3945, 0.3945],\n",
    "              test_loss: [1.2641, 1.5706],\n",
    "              test_acc: [0.3400, 0.2973]}\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Make sure model on target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Setup training and save the results\u001b[39;00m\n\u001b[1;32m     10\u001b[0m results \u001b[38;5;241m=\u001b[39m train(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m---> 11\u001b[0m                        train_dataloader\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataloader\u001b[49m,\n\u001b[1;32m     12\u001b[0m                        test_dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m     13\u001b[0m                        optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     14\u001b[0m                        loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     15\u001b[0m                        epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     16\u001b[0m                        device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# End the timer and print out how long it took\u001b[39;00m\n\u001b[1;32m     19\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Setup training and save the results\n",
    "results = train(model=model,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=5,\n",
    "                       device=device)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/ohpc/pub/apps/python3/3.11.5/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.4494808774324693\n",
      "Epoch 2/5, Loss: 0.26771371982400605\n",
      "Epoch 3/5, Loss: 0.2114097026211906\n",
      "Epoch 4/5, Loss: 0.17174471914827805\n",
      "Epoch 5/5, Loss: 0.1481803754107432\n",
      "Test Accuracy: 93.28%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Define the data transformations for training and testing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # EfficientNet models typically expect 224x224 input images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root=\"data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=\"data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load a pretrained EfficientNet model\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Modify the classifier layer for 10 classes (CIFAR-10 has 10 classes)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
